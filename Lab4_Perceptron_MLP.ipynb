{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.8.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 — Perceptron & Multi-Layer Perceptron (MLP)\n",
    "**Name:** Jiya Raval &nbsp;|&nbsp; **USN:** 1AUA23BCS078 &nbsp;|&nbsp; **Class:** CSE-B2 &nbsp;|&nbsp; **Sem:** 6\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 — Single Layer Perceptron (from scratch)\n",
    "**Objective:** Train a perceptron to classify points based on a linear boundary using Python from first principles.\n",
    "\n",
    "**Steps:**\n",
    "1. Setup Python Environment\n",
    "2. Define Perceptron Function\n",
    "3. Generate Synthetic Training Data\n",
    "4. Train with Perceptron Learning Rule\n",
    "5. Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Training Data ---\n",
    "X = np.array([[2,3],[1,1],[4,5],[6,7],[3,2],[7,8]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# --- Initialize weights, bias, hyperparameters ---\n",
    "w  = np.zeros(2)\n",
    "b  = 0\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "\n",
    "# --- Perceptron Learning Rule ---\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "        linear_output = np.dot(X[i], w) + b\n",
    "        prediction    = 1 if linear_output >= 0 else 0\n",
    "        error         = y[i] - prediction\n",
    "        w += lr * error * X[i]\n",
    "        b += lr * error\n",
    "\n",
    "print(\"Learned Weights:\", w)\n",
    "print(\"Learned Bias   :\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Decision Boundary ---\n",
    "colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "for i in range(len(X)):\n",
    "    plt.scatter(X[i][0], X[i][1], color=colors[i], s=80, zorder=5)\n",
    "\n",
    "x_vals = np.linspace(0, 8, 100)\n",
    "y_vals = -(w[0] * x_vals + b) / w[1]\n",
    "\n",
    "plt.plot(x_vals, y_vals, color='green', linewidth=2, label='Decision Boundary')\n",
    "plt.scatter([], [], color='red',  label='Class 0')\n",
    "plt.scatter([], [], color='blue', label='Class 1')\n",
    "plt.title(\"Task 1: Single Layer Perceptron\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Implemented a single layer perceptron from scratch using NumPy.\n",
    "- The model correctly separates linearly separable data using a straight decision boundary.\n",
    "- A single layer perceptron **cannot** solve non-linear problems like XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 — Multi-Layer Perceptron (MLP) from scratch\n",
    "**Objective:** Build an MLP classifier using NumPy to learn non-linear decision boundaries (XOR problem).\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer  : 2 neurons\n",
    "- Hidden layer : 4 neurons (ReLU activation)\n",
    "- Output layer : 1 neuron (Sigmoid activation)\n",
    "\n",
    "**Steps:** Define activations → Initialize weights → Forward pass → Backpropagation → Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Activation Functions ---\n",
    "def relu(x):         return np.maximum(0, x)\n",
    "def relu_grad(x):    return (x > 0).astype(float)\n",
    "def sigmoid(x):      return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_grad(x): return x * (1 - x)\n",
    "\n",
    "# --- XOR Dataset ---\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# --- Initialize Weights & Biases (hidden=4) ---\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 4) * 0.5\n",
    "b1 = np.zeros((1, 4))\n",
    "W2 = np.random.randn(4, 1) * 0.5\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "lr     = 0.1\n",
    "epochs = 5000\n",
    "losses = []\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    output = sigmoid(z2)\n",
    "\n",
    "    # Loss (MSE)\n",
    "    loss = np.mean((y - output) ** 2)\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    d_out    = (output - y) * sigmoid_grad(output)\n",
    "    dW2      = np.dot(a1.T, d_out)\n",
    "    db2      = np.sum(d_out, axis=0, keepdims=True)\n",
    "    d_hidden = np.dot(d_out, W2.T) * relu_grad(z1)\n",
    "    dW1      = np.dot(X.T, d_hidden)\n",
    "    db1      = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Weight update\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "print(\"Final Predictions (Hidden=4):\")\n",
    "for inp, pred in zip(X, output):\n",
    "    print(f\"  Input: {inp}  →  Output: {pred[0]:.4f}  →  Predicted: {int(pred[0] > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot Loss ---\n",
    "plt.plot(losses, color='steelblue')\n",
    "plt.title(\"Task 2: Loss vs Epochs (Hidden=4)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- MLP with 4 hidden neurons successfully learns the XOR problem.\n",
    "- Loss decreases steadily over epochs, confirming backpropagation is working.\n",
    "- Final predictions are very close to expected values `[0, 1, 1, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 — Comparison & Analysis\n",
    "1. Compare Single Layer vs MLP on XOR (non-linear data)\n",
    "2. Explore how hidden layer size impacts learning\n",
    "3. Plot loss over training epochs for different hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: train MLP and return final loss + predictions ---\n",
    "def train_mlp(hidden_size, epochs=5000, lr=0.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.randn(2, hidden_size) * 0.5\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, 1) * 0.5\n",
    "    b2 = np.zeros((1, 1))\n",
    "    losses = []\n",
    "    for _ in range(epochs):\n",
    "        z1 = np.dot(X, W1) + b1;  a1 = relu(z1)\n",
    "        z2 = np.dot(a1, W2) + b2;  o  = sigmoid(z2)\n",
    "        losses.append(np.mean((y - o) ** 2))\n",
    "        d_out    = (o - y) * sigmoid_grad(o)\n",
    "        dW2      = np.dot(a1.T, d_out)\n",
    "        db2      = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_hidden = np.dot(d_out, W2.T) * relu_grad(z1)\n",
    "        W2 -= lr * dW2;  b2 -= lr * db2\n",
    "        W1 -= lr * np.dot(X.T, d_hidden)\n",
    "        b1 -= lr * np.sum(d_hidden, axis=0, keepdims=True)\n",
    "    return losses, o\n",
    "\n",
    "# --- Compare hidden sizes: 2, 4, 8 ---\n",
    "hidden_sizes = [2, 4, 8]\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for idx, hs in enumerate(hidden_sizes):\n",
    "    loss_curve, preds = train_mlp(hs)\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.plot(loss_curve, color=['orange','steelblue','green'][idx])\n",
    "    plt.title(f\"Hidden Size = {hs}\\nFinal Loss: {loss_curve[-1]:.4f}\")\n",
    "    plt.xlabel(\"Epochs\"); plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True)\n",
    "    print(f\"\\n--- Hidden Size = {hs} ---\")\n",
    "    for inp, p in zip(X, preds):\n",
    "        print(f\"  {inp}  →  {p[0]:.4f}  (pred: {int(p[0]>0.5)})\")\n",
    "\n",
    "plt.suptitle(\"Task 3: Effect of Hidden Layer Size on XOR Learning\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Conclusions\n",
    "\n",
    "| Model | XOR Problem | Linear Data |\n",
    "|-------|------------|-------------|\n",
    "| Single Layer Perceptron | ❌ Cannot solve | ✅ Works perfectly |\n",
    "| MLP (hidden=2) | ⚠️ Struggles | ✅ Works |\n",
    "| MLP (hidden=4) | ✅ Solves well | ✅ Works |\n",
    "| MLP (hidden=8) | ✅ Solves best | ✅ Works |\n",
    "\n",
    "- **Single Layer Perceptron** works only for linearly separable data.\n",
    "- **MLP** with backpropagation can learn non-linear patterns like XOR.\n",
    "- **Increasing hidden neurons** improves learning capacity but adds computation cost.\n",
    "- **Hidden layers** allow the model to learn complex, non-linear decision boundaries."
   ]
  }
 ]
}
